{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiiyer/ERA1/blob/main/Abhishek_ERA_V1_S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "694c1eb4-4d8e-4570-92a7-5c0ec8d5c6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn # torch neural network\n",
        "import torch.nn.functional as F # torch functions\n",
        "import torch.optim as optim # optimizer\n",
        "from torchvision import datasets, transforms # datasets and transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available() # check if nvidia cuda gpu is available\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "916f2afe-74b6-4510-cb5c-7b8d5154e5d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the batch size (normally 2^x)\n",
        "batch_size = 128 \n",
        "\n",
        "# load the train data and perform standard normalization\n",
        "# Normalize does the following for each channel:\n",
        "# image = (image - mean) / std\n",
        "# The parameters mean, std\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# load the test data and perform standard normalization\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54d585d-f738-4936-811a-80d30aec864c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 352796965.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 92611386.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 130527921.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15640828.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far. \n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\". "
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"nn.Module: Base class for all neural network modules.\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n",
        "\"\"\"\n",
        "\n",
        "class FirstDNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FirstDNN, self).__init__()\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # input - 1x28x28   | output - 32x28x28     | RF - 3x3\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # input - 32x28x28  | output - 64x28x28     | RF - 5x5\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)  # input - 64x28x28  | output - 64x14x14     | RF - 10x10\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)  # input - 64x14x14  | output - 128x14x14    | RF - 12x12\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)   # input - 128x14x14 | output - 256x14x14    | RF - 14x14\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)  # input - 256x14x14 | output - 256x7x7      | RF - 28x28\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)  # input - 256x7x7   | output - 512x5x5      | RF - 30x30\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)  # input - 512x5x5   | output - 1024x3x3     | RF - 32x32\n",
        "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out: \n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)  # input - 1024x3x3  | output - 10x1x1       | RF - 34x34\n",
        "\n",
        "    # self.fc1 = nn.Linear(9216, 128)\n",
        "    # self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "  \"\"\"forward\n",
        "    Defines the computation performed at every call.\n",
        "\n",
        "    Args:\n",
        "        x: the input\n",
        "\n",
        "    Returns:\n",
        "        log_softmax(x)\n",
        "    \"\"\"\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))   # performs conv1 -> relu -> conv2 -> relu -> pool1\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))   # performs conv3 -> relu -> conv4 -> relu -> pool2\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))               # performs conv5 -> relu -> conv6 -> relu\n",
        "    x = F.relu(self.conv7(x))                                   # performs conv7 \n",
        "    x = x.view(-1, 10)                                          # similar to reshape in numpy\n",
        "\n",
        "    # x = torch.flatten(x, 1)\n",
        "    # x = F.relu(self.fc1(x))\n",
        "    # x = self.fc2(x)\n",
        "\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "    # never use ReLU in the last layer, the model cannot laern negative values just by removing F.relu() we get 99% accuracy.\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstDNN().to(device) # transfer the model to the device chosen above"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the summary of the model\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "f3f9e2cd-f264-4dca-c161-c1a07d371069"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-821ce71605c6>:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\"\"\"trains the model\n",
        "\n",
        "Args\n",
        "    model: the model to be trained\n",
        "    device: the device on which to be trained, cpu/gpu\n",
        "    train_loader: the train data loader from torch.utils.data.DataLoader\n",
        "    optimizer: the optimizer to use for training\n",
        "    epoch: the number of epoch to run for\n",
        "\n",
        "Returns\n",
        "    None\n",
        "\"\"\"\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # set the model on train mode\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)  # move the data to the device\n",
        "        optimizer.zero_grad()          # zero the gradients\n",
        "        output = model(data)    # get the model output for the data\n",
        "        loss = F.nll_loss(output, target)  # loss is negative log likelihood\n",
        "        loss.backward()   # flow the gradients backward\n",
        "        optimizer.step()   # optimizer.step is performs a parameter update based on the current gradient \n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')   # this is just for pretty printing\n",
        "\n",
        "\"\"\"tests the model\n",
        "\n",
        "Args\n",
        "    model: the model to test\n",
        "    device: the device to use\n",
        "    test_loader: the test data loader from torch.utils.data.DataLoader\n",
        "\"\"\"\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # set the model on eval mode\n",
        "    test_loss = 0      # set the test loss to zero\n",
        "    correct = 0     # number of correct classifications\n",
        "    \n",
        "    # turn off gradients, since we are in test mode\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)  # move the data to device\n",
        "            output = model(data)  # get the model output\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # stochastic gradient descent with model parameters, learning rate and momentum\n",
        "\n",
        "# run the model for range number of times\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "4c104172-fae2-413e-c4fb-e024c8acece0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-6-821ce71605c6>:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.5803605914115906 batch_id=468: 100%|██████████| 469/469 [00:31<00:00, 14.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7298, Accuracy: 7015/10000 (70%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reIBU667OG_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}